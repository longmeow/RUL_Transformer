{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7XMhKM3hEZCZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retry attempt failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno -3] Temporary failure in name resolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 976, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connection.py\", line 308, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f15c49e6e80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 724, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/urllib3/util/retry.py\", line 439, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f15c49e6e80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/wandb/sdk/lib/retry.py\", line 113, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 213, in execute\n",
      "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/transport/requests.py\", line 38, in execute\n",
      "    request = requests.post(self.url, **post_args)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/quanhhh/miniconda3/envs/fedml-torch/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f15c49e6e80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otiUHA1UvLUp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpGye-k7w0s_"
   },
   "outputs": [],
   "source": [
    "with open('/home/quanhhh/Documents/RUL_Transformer/bo.yml') as f:\n",
    "  sweep_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8pu_VZPvLe1"
   },
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dff = dff\n",
    "        self.dropout = dropout\n",
    "        self.encoderlayer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dff,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        out = self.encoderlayer(src)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.num_layer = num_layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = self.transformer_encoder(src)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, linear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = F.relu(self.linear(self.encoder(src)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def create_transformer(d_model, nhead, dff, num_layers, dropout, l_win):\n",
    "    linear = nn.Sequential(\n",
    "        nn.Flatten(), nn.Dropout(dropout), nn.Linear(d_model * l_win, 1)\n",
    "    )\n",
    "    model = TransformerModel(\n",
    "        TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model, nhead, dff, dropout), num_layers\n",
    "        ),\n",
    "        linear,\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "      if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "#dataloader.py\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, config, mode):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.load_dataset(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            data = self.data[idx, :, :]\n",
    "            label = self.label[idx]\n",
    "            return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def load_dataset(self, config):\n",
    "        if self.mode == 'train':\n",
    "            train_df = pd.read_csv('/home/quanhhh/Documents/RUL_Transformer/preprocessed_data/train_004.csv')\n",
    "\n",
    "            def gen_sequence(id_df, seq_length, seq_cols):\n",
    "                data_array = id_df[seq_cols].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "                    yield data_array[start:stop, :]\n",
    "            \n",
    "            sensor_cols = [\n",
    "                \"s1\",\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s5\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s16\",\n",
    "                \"s17\",\n",
    "                \"s18\",\n",
    "                \"s19\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            sequence_cols.extend(sensor_cols)\n",
    "            # generator for the sequences\n",
    "            seq_gen = (list(gen_sequence(train_df[train_df['id']==id], self.config['l_win'], sequence_cols)) \n",
    "                      for id in train_df['id'].unique())\n",
    "\n",
    "            # generate sequences and convert to numpy array\n",
    "            seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "            # function to generate labels\n",
    "            def gen_labels(id_df, seq_length, label):\n",
    "                data_array = id_df[label].values\n",
    "                num_elements = data_array.shape[0]\n",
    "                return data_array[seq_length:num_elements, :]\n",
    "\n",
    "            # generate labels\n",
    "            label_gen = [gen_labels(train_df[train_df['id']==id], self.config['l_win'], ['RUL']) \n",
    "                        for id in train_df['id'].unique()]\n",
    "            label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array\n",
    "            self.label = label_array\n",
    "\n",
    "        else:\n",
    "            test_df = pd.read_csv('/home/quanhhh/Documents/RUL_Transformer/preprocessed_data/test_004.csv')\n",
    "\n",
    "            sensor_cols = [\n",
    "                \"s1\",\n",
    "                \"s2\",\n",
    "                \"s3\",\n",
    "                \"s4\",\n",
    "                \"s5\",\n",
    "                \"s7\",\n",
    "                \"s8\",\n",
    "                \"s9\",\n",
    "                \"s10\",\n",
    "                \"s11\",\n",
    "                \"s12\",\n",
    "                \"s13\",\n",
    "                \"s14\",\n",
    "                \"s15\",\n",
    "                \"s16\",\n",
    "                \"s17\",\n",
    "                \"s18\",\n",
    "                \"s19\",\n",
    "                \"s20\",\n",
    "                \"s21\",\n",
    "            ]\n",
    "            sequence_cols = [\"setting1\", \"setting2\", \"setting3\"]\n",
    "            sequence_cols.extend(sensor_cols)\n",
    "\n",
    "            seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-config['l_win']:] \n",
    "                                  for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= config['l_win']]\n",
    "\n",
    "            seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "\n",
    "            y_mask = [len(test_df[test_df['id']==id]) >= config['l_win'] for id in test_df['id'].unique()]\n",
    "\n",
    "            label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "            label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "\n",
    "            self.data = seq_array_test_last\n",
    "            self.label = label_array_test_last\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trainer.py\n",
    "class ModelTrainer():\n",
    "    def __init__(self, model, train_data, criterion, optimizer, device, config):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_loss_list = list()\n",
    "        self.min_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        train_loss = 0.0\n",
    "        self.model.train()\n",
    "        for x, rul in self.train_data:\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x.to(self.device).float())\n",
    "            loss = torch.sqrt(self.criterion(out.float(), rul.to(self.device).float()))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "            \n",
    "\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        wandb.log({\"train loss\": train_loss})\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < self.min_loss:\n",
    "            self.min_loss = train_loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "            self.best_optimizer = deepcopy(self.optimizer.state_dict())\n",
    "            self.best_epoch_in_round = epoch\n",
    "\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(1, self.config['n_epochs'] + 1):\n",
    "            self.train_epoch(epoch)\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "\n",
    "\n",
    "        self.config['train_loss_list'] = self.train_loss_list\n",
    "\n",
    "    def update_config(self):\n",
    "        return self.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo9qmgHw0q6c"
   },
   "outputs": [],
   "source": [
    "#train.py\n",
    "def training():\n",
    "    with wandb.init(config=sweep_config):\n",
    "        config = wandb.config\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_data = TimeSeriesDataset(config, mode='train')\n",
    "        train_loader = DataLoader(train_data,\n",
    "                                  batch_size=config['batch_size'],\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=config['num_workers'])\n",
    "\n",
    "        model = create_transformer(d_model=config['d_model'],\n",
    "                                    nhead=config['n_head'],\n",
    "                                    dff=config['dff'],\n",
    "                                    num_layers=config['num_layers'],\n",
    "                                    dropout=config['dropout'],\n",
    "                                    l_win=config['l_win'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.MSELoss()\n",
    "        trainer = ModelTrainer(model, train_loader, criterion, optimizer, device, config)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        #inference.py\n",
    "\n",
    "        test_data = TimeSeriesDataset(config, mode='test')\n",
    "        test_loader = DataLoader(test_data,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=config['num_workers'])\n",
    "\n",
    "        model.to(device)\n",
    "        test_loss = 0.0\n",
    "        criterion = nn.MSELoss()\n",
    "        test_loss_list = list()\n",
    "        pred_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in test_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                test_loss += loss\n",
    "                test_loss_list.append(loss)\n",
    "                pred_list.append(out.float())\n",
    "\n",
    "        test_loss_avg = test_loss / len(test_loader)\n",
    "        truth_list = [rul.float().item() for x, rul in test_loader]\n",
    "        config['truth_list'] = truth_list\n",
    "        config['pred_list'] = pred_list\n",
    "        config['test_loss_avg'] = test_loss_avg\n",
    "        config['test_loss_list_per_id'] = test_loss_list\n",
    "        wandb.log({\"test_loss_avg\": test_loss_avg})\n",
    "        if test_loss_avg < 12.4:\n",
    "            wandb.alert(\n",
    "                title='longmeow come first with ey2demty',\n",
    "                text=f'test_loss_avg {test_loss_avg} is below the theshold 12.4',\n",
    "            )\n",
    "            print('Alert triggered')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XywYZZplSBha"
   },
   "outputs": [],
   "source": [
    "!date\n",
    "wandb.agent('rul-project/6so07szz', training, count=100000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5284041c465f1e9ad5b1e62efda0b3d1789ef7eb45be627d8c9bdc79622ee5b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
